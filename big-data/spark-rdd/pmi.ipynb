{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark assignment 2: Collocations\n",
    "\n",
    "As for the second part of the assignment, your task is to extract collocations: that is word combinations that occur together. For example, “high school” or “roman empire”.\n",
    "\n",
    "To find collocations, you will use NPMI (normalized pointwise mutual information) metric.\n",
    "\n",
    "PMI of two words, a & b, is defined as “PMI(a, b) = ln (P(ab) / (P(a) * P(b))”, where P(ab) is the probability of two words coming one after the other, and P(a) and P(b) are probabilities of words a & b respectively.\n",
    "\n",
    "You will estimate probabilities with occurrence counts, that is “P(a) = # of occurrences of word a / total number of words”, and “P(ab) = # of occurrences of words ‘a b’ / total number of word pairs”.\n",
    "\n",
    "Therefore, rare combinations of coupled words have large PMI.\n",
    "\n",
    "NPMI is computed as “NPMI(a, b) = PMI(a, b) / -ln P(ab)”. This normalizes the quantity to be within the range [-1; 1].\n",
    "\n",
    "You task is a bit more complicated now:\n",
    "\n",
    "* Extract all the words, as in the previous task.\n",
    "* Filter out stopwords using the dictionary (/datasets/stop_words_en.txt ) (do not forget to convert words to the lowercase!)\n",
    "* Compute all bigrams (that is, pairs of consequent words)\n",
    "* Leave only bigrams with at least 500 occurrences\n",
    "* Compute NPMI for every bigram (note: when computing probabilities, you need unpruned counts!)\n",
    "* Sort word pairs by NPMI in the descending order\n",
    "* Print top 39 word pairs, with words delimited by the underscore “_”\n",
    "\n",
    "For example,\n",
    "\n",
    "    roman_empire\n",
    "    south_africa\n",
    "    \n",
    "Dataset location: /data/wiki/en_articles_part\n",
    "\n",
    "The part of the result on the sample dataset:\n",
    "\n",
    "    ...\n",
    "    references_reading\n",
    "    notes_references\n",
    "    award_best\n",
    "    north_america\n",
    "    new_zealand\n",
    "    ...\n",
    "    \n",
    "Hint: if you did everything right, “roman_empire” and “south_africa” are going to be in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python2 on grader, Python3 in docker\n",
    "import sys\n",
    "if sys.version_info.major == 3:\n",
    "    import os\n",
    "    os.environ['PYSPARK_PYTHON'] = '/opt/conda/bin/python'\n",
    "    os.environ['PYTHONHASHSEED'] = '42'\n",
    "    unicode = str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"PMI\").setMaster(\"local\"))\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_article(line):\n",
    "    try:\n",
    "        article_id, text = unicode(line.rstrip()).split('\\t', 1)\n",
    "        text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "        return words\n",
    "    except ValueError as e:\n",
    "        return []\n",
    "\n",
    "wiki = sc.textFile(\"/data/wiki/en_articles_part/articles-part\", 16).map(parse_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase texts\n",
    "\n",
    "lwiki = wiki.map(lambda text: [s.lower() for s in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = lwiki.take(1)[0]\n",
    "#for word in result[:5]:\n",
    "#    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(fname):\n",
    "    with open(fname) as h:\n",
    "        return [unicode(l.strip()) for l in h]\n",
    "\n",
    "STOP_WORDS = load_stopwords('/datasets/stop_words_en.txt')\n",
    "# print(STOP_WORDS[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(words):\n",
    "    return [w for w in words if w not in STOP_WORDS]\n",
    "texts = lwiki.map(filter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = texts.take(1)[0]\n",
    "#for word in result[:5]:\n",
    "#    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_list_to_pairs(ls):\n",
    "    return zip(ls, ls[1:])\n",
    "\n",
    "pairs = texts.flatMap(word_list_to_pairs)\n",
    "pairs = pairs.map(lambda p: (p[0] + '_' + p[1], 1))\n",
    "pairs.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leave only bigrams with at least 500 occurrences\n",
    "\n",
    "sumFunc = lambda x, y: x + y\n",
    "\n",
    "counted_pairs = pairs.aggregateByKey(0, sumFunc, sumFunc)\n",
    "counted_pairs.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs500 = counted_pairs.filter(lambda rec: rec[1] >= 500)\n",
    "pairs500.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs500.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute NPMI for every bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word counts\n",
    "\n",
    "words = texts.flatMap(lambda text: [(w, 1) for w in text])\n",
    "counted_words = words.aggregateByKey(0, sumFunc, sumFunc)\n",
    "counted_words.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# counted_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join for the first word\n",
    "# (bigr, Nb) -> (left_word, (bigr, Nb)) -> join against (word, nw) -> (left_word, (bigt, Nb, nw))\n",
    "def first_word_as_key(rec):\n",
    "    (key, nb) = rec\n",
    "    return (key.split('_')[0], (key, nb))\n",
    "for_join1 = pairs500.map(first_word_as_key)\n",
    "\n",
    "# for_join1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join1 = for_join1.join(counted_words)\n",
    "join1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join for the first word\n",
    "# ('notes', (('notes_references', 638), 2267)) -> ('references', (('notes_references', 638), 2267))\n",
    "def second_word_as_key(rec):\n",
    "    (_, ((key, nb), nl)) = rec\n",
    "    return (key.split('_')[1], (key, nb, nl))\n",
    "for_join2 = join1.map(second_word_as_key)\n",
    "\n",
    "#for_join2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[29] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join2 = for_join2.join(counted_words)\n",
    "join2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute NPMI\n",
    "# Using counts instead of probabilities because doesn't affect sorting\n",
    "import math\n",
    "\n",
    "# ('references', (('notes_references', 638, 2267), 4151)),\n",
    "def calc_npmi(rec):\n",
    "    (_, ((pair, nb, nl), nr)) = rec\n",
    "    pmi = math.log(1.0 * nb / nl / nr)\n",
    "    npmi = -1.0 * pmi / math.log(nb)\n",
    "    return (pair, npmi)\n",
    "npmi = join2.map(calc_npmi)\n",
    "\n",
    "# npmi.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort word pairs, print top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[55] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npmi_sorted = npmi.sortBy(lambda rec: rec[1])\n",
    "npmi_sorted.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los_angeles\n",
      "external_links\n",
      "united_states\n",
      "prime_minister\n",
      "new_york\n",
      "san_francisco\n",
      "19th_century\n",
      "et_al\n",
      "20th_century\n",
      "supreme_court\n",
      "references_external\n",
      "soviet_union\n",
      "air_force\n",
      "university_press\n",
      "united_kingdom\n",
      "world_war\n",
      "baseball_player\n",
      "war_ii\n",
      "roman_catholic\n",
      "north_america\n",
      "civil_war\n",
      "new_zealand\n",
      "notes_references\n",
      "references_reading\n",
      "award_best\n",
      "american_actor\n",
      "catholic_church\n",
      "united_nations\n",
      "south_africa\n",
      "took_place\n",
      "roman_empire\n",
      "american_actress\n",
      "high_school\n",
      "american_singer-songwriter\n",
      "american_baseball\n",
      "york_city\n",
      "american_football\n",
      "years_later\n",
      "north_american\n"
     ]
    }
   ],
   "source": [
    "for rec in npmi_sorted.take(39):\n",
    "    print(rec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
